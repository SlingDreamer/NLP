Abstract  
//We propose MCWE,a novel method for learning Chinese word embeddings.
Word embeddings have a great impact on natural language processing.
In morpheme writing systems,most Chinese word embeddings takes a word as the basic unit, or directly use the internal structure of words.
However,these models still neglect the rich latent meanings in the internal structure of Chinese characters.
In this paper,we explore to employ the latent meanings of the main-components of the Chinese characters to train and enhance the Chinese word embeddings.
Based on this purpose,we propose three main-component enhanced word embedding models(MCWEs),named MCWE-Avg,MCWE-SA,MCWE-HA respectively,
which incorporate the latent meanings of the main-components during the training process.
Experiments on word similarity, syntactic analogy and text classification are conducted to validate the feasibility of our models.
Evaluations on both word similarity and word analogy tasks demonstrate the superior performance of our models.

1.Introduction
  Word embedding, which is also termed distributed word representation, has been an important foundation in the field of Natural Language Processing (NLP).
It encodes the semantic meaning of a word into a real-valued low-dimensional vector,which performs better in many tasks such as text classification, machine translation ..... (and so on) over traditional one-hot representations.
Among many word embedding models,Continuous Bag-of-Word (CBOW), Skip-gram (Mikolov et al.,2013a), Global Vectors (GloVe) (Penningtonet al., 2014) is popular because of their simplicity and efficiency.
(分布式假说)(morphemes/characters)
However, these models only focus on word level information, and do not pay attention to the fine-grained morphological information inside the words or the characters,such as componets of Chinese character or English
morphemes.
    Different from English words,Chinese characters are glyphs whose components may depict objects or represent abstract notions.Usually a character consists of more commonly two or more components 
which have meanings close to the character, using a variety of different principles[wiki written chinese https://en.wikipedia.org/wiki/Written_Chinese]
That means Chinese words themselves are often composed of Chinese characters and subcharacter compoents,including rich semantic information.
    (引用论文)（过去式）
  
  Previous researchers have done some works by using the rich information inside Chinese for word embeddings enhancement with internal morphological semantics.
   (Li et al.  Component-Enhanced Chinese Character Embeddings) used the radicals to enhance the Chinese character embeddings.
(Chen et al. Joint Learning of Character and Word Embeddings) proposed the CWE model to improve the quality of Chinese word embeddings by exploit character level information.   
For a more fine-grained combination of Chinese character and radical,(Yin et al.Multi-Granularity Chinese Word Embedding)proposed methods to enhance Chinese character embeddings based on CWE model.
   (Jian et al.Improve Chinese Word Embeddings by Exploiting Internal Structure) used external language to calculate the similarity between Chinese words and characters to enhance semantic information
  based on the rich internal structure of Chinese words
(Monday4.22)
//In this work, we present a model to jointly learn the embeddings of Chinese words, characters,and subcharacter components.

(huang er al. Learning Chinese Word Representations From Glyphs Of Characters)proposed the GWE model,a pixel-based model that learns character features from font images to enhance the representation (of words with character glyphs).
(Yu et al.Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components) used Chinese characters and subcharacter components to improve Chinese word embeddings 
and proposed the JWE model to jointly learn Chinese word and character embeddings.
(Cao et al. cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information)proposed a cw2vec model, which exploits stroke-level information to improve the learning of Chinese word embeddings.


   However, the subcharacter compoents of a character contain a lot of unwanted noise information, so we integrate the word components into main-component.
Chinese characters contain elements and radicals，Just as Chinese contains Chinese characters.
The components of characters can be roughly divided into two types: main-component and radical.The main-component which consists of several components indicates the baisc meaning of a character while the radical indicates some attribute meanings of a character.
For example,"智 (intelligence)” is reduced into the components “矢(arrow)”, “口(mouth)” and “日(sun)”,and 矢(arrow) and 口(mouth) and are the components and 日(sun) is the radical of charactor 智.All these components may not be relevant to the semantics of the character.However, the main-component "知(wisdom)" consisting of several small components is closely related to the meaning of the charactor. Based on this, we have proposed .....

    In this paper, we have modified the CBOW model in the word2vec source code, introduced the information of the latent meaning of the Chinese main-component and radical, 
and proposed three efficient models called MCEW-Avg, MCWE-SA, and MCWE-HA.
   The learned Chinese word embeddings not only encode morphological properties into Chinese words, but also have a higher similarity to the synonym.
   Our model directly modifies embeddings of the target words, without generating and training extra embeddings for latent meanings.
In addition,we create a word map to describe the relationship between Chinese words and the latent meanings of their main-components and radicals.


Through our model, the rich implicit information in Chinese is fully utilized and the similarities between words and words is improved.
//Using our model, we can strengthen the similarity between words and words more effectively.
（贡献）1.构建汉语主成分map表；2.attention机制引用 3.有很强的拓展性（CWE）
Our contributions of this paper can be summarized as follows:

1.Rather than directly leverage the components of the word itself, we provide a method to use the latent meanings of the main-components and radicals to train the word embedding.
In order to verify the feasibility of our purpose,two models,named MCEW-SA and MCWE-HA,are proposed to incorporate the latent meanings.

2.We propose a method to assign the weights of latent meanings at input layer based on the attention scheme.
Through the attention mechanism, we focus on the latent meanings of higher contribution to the target word and assign them higher weights.

*******************************


2.My model(map表需要精确的标注隐含词义,对模型影响很大)
In this section, we introduce our Main-Component Word Embedding models(MCWEs),named Main-Component Word Embedding-Average(MCWE-Avg),MCWE-Soft Attention(MCWE-SA),MCWE-Hard Attention(MCWE-HA),
which is based on CBOW model(Mikolov et al., 2013a).
It should be noted that our models directly uses the internal meanings of the Chinese characters,rather than directly using the characters or componets of the word themselves.
MCWE-Avg assumes that all the latent meanings of the Chinese character's main-component to have the same weight.
However, most of the main-components are also a frequently-used Chinese word which may contain some ambiguous information.
To address this concern,we proposed the MCWE-SA which which based on the soft attention scheme.
The MCWE-SA assumes that the latent meanings of main-components have their own weights,and assign higher weights to the meanings closely related to the target character,
so that they have greater impact on the word embeddings.
What's more,MCWE-HA which based on the hard attention scheme only focuses on the latent meaning of the main-component with the highest similarity to the target word.
In what follows, we will introduce each of our MCWEs in detail.
At the end of this section, we are going to introduce the update rules of the models.

MCWE-Avg:

We denote the Chinese character set as C and the Chinese word vocabulary as W. Each character ci-C is represented by vector ci, 
and each word wi-W is represented by vector wi.

We denote D as the training corpus, W =(w1;w2;.....;wN) as the vocabulary of words,C = (c1; c2;.... ; cM) as the vocabulary of characters. 

The item for wi in the word map is wi -- Mi. Mi is a set of latent meanings of wi's main-components, 
(Particularly,we treat the radical as a main-component and add the latent meaning of the radical to the main-component latent meaning.)
Formally, a context word wj is represented as
公式见本子


MCWE-SA：

Through observation, we found that most Chinese words have more than one latent meaning with their main-component,
but some latent meanings have low correlation with target words(the corresponding word).For example,main-component "知(knowledge)" means "" 、"" and "".
As Fig.3 shows,for the item "智慧(intelligence)" ——{[],[],[],[],[],[],[],[]} in the word map,each latent meaning has a bias on the word "智慧(intelligence)".
Therefore, we assign different weights to each latent meaning based on the idea of soft attention model.
We measure the weights of latent meanings by calculating the cosine similarity between the corresponding latent meanings and the target word where cosine similarity is usually used to measure the similarity between word embeddings.
Furthermore,we remove the negatively correlated latent meaning.
Hence, at the input layer, the modified embedding of xi can be expressed as

公式见本
Let Sim(·) denotes the function to calculate the similarity between meanings of Chinese words and characters, we use cosine distance as the distance metric.
The i-th latent meaning of the main-component of Chinese character c are ci.


MCWE-HA:
//In order to further reduce the problem that the main component implies the opposite of the similarity of Chinese characters, we propose MCWE-HA.
In order to further reduce the impact of irrelevant latent meanings to the word, we propose MCWE-HA which is based on the idea of hard attention model.
We only choose the latent meaning which have the greatest similarity to the Chinese word.
According to experimental experience, we only retain the latent meaning of similarity with token xi greater than 0.5.
And MCWE-HA is mathematically defined as
公式（3）

where we only keep values with cos（Vxi,Vm） greater than 0.5 for comparison. 



*******************************
3.Experiments
In this section,we test the performance of our model in generating high-quality word embeddings on word similarity evaluation and word analogy tasks.

3.1 Experimental Settings

Traing Corpus : 
We utilize a medium-sized Chinese corpus to train all word embedding models.We adopt the Chinese Wikipedia Dump as our training corpus whose size is about 7GB.
We use a script named WikiExtractor to convert data from XML into text format.What's more,We use THULAC 3 (Sun et al.,2016b) for Chinese word segmentation and normalize all characters as simplified Chinese.
In pre-processing,we filter all digits,punctuation masks and non Chinese characters.
To get better quality of the word embeddings, we removed the stop words (that are common_ which from *********) in the corpus.
Finally, we obtained a training corpus of approximately 2g in size, containing 2123123 words, and 213 unique words.

Word Map:
The Modern Chinese Word List is divided into two parts: 2500 common characters and 1000 secondary common characters,and the coverage of those words in most corpora reached 99.48% which means mastering the common and secondary words has reached the basic requirements for using Chinese.
Hence,we use crawler script to obtain the main-components and radicals information of those Chinese characters which is a total of 3500 from HTTPCN. We obtained 3433 main-components and 57 radicals.
//we obtained 20,879 characters, 13,253 components and 218 radicals,of which 7,744 characters have more than one components, and 214 characters are equal to their radicals.
To create the word map,we need to obtain the explanatory meaning of the main-components of each characters.
Although the main-components are part of the characters, they are themselves characters.
Therefore, we have crawled the Chinese interpretation of 3433 main-component characters from HTTPCN and obtained the core latent meanings using manual annotation.
Similarly, we obtained the core latent meaning of 57 radicals from XXX.
Although this process costs manpower and time, it can be done once and for all for each languag because it has the same knowledge base.
When we choose a Chinese charactor,its main-component and radical will be selected and further replaced by their latent meanings.

(为了更快的训练速度 我们的模型采用了CBOW作为基础)

Parameter Settings:
(-cbow 1 -size 200 -window 5 -negative 10 -hs 0 -sample 1e-4 -threads 25 -binary 0 -iter 15 -alpha 0.025)
For the sake of fairness, we used the same parameter settings for all models.
In order to speed up the training process, we have adopted negative sampling techniques for both Cbow and skip-gram and our models.

We set the word vector dimension as 200, the window size as 5, the training iteration as 100, the initial learning rate as 0.025, and the subsampling parameter to be 1e-4. Words with
frequency less than 5 were ignored during training. We used 10-word negative sampling for optimization.

4.Evaluation Benchmarks
4.4.1 Word Similarity

This experiment is used to evaluate the semantic relevance of generated word embeddings in word pairs.

For Chinese word similarity, we employ two differnet manually-annotated datasets,wordsim-240 and wordsim-296 provided by (Chen et al., 2015).
These datasets are composed of word pairs and are manually labeled with the similarity scores for each word pair.
We utilize the cosine similarity to measure the similarity of each word pair, and the Spearman’s rank correlation coefficient (p) is employed to evaluate our calculation results and human scores.
More details of these datasets are shown in Table 1.


4.4.2 Syntactic Analogy

This task examines the quality of word embeddings by discovering the semantic inferential capability between pairs of words.
The core task of syntactic analogy is to answer the questions like "雅典(Athens) is to 希腊(Greece) as 东京(Tokyo) is to 日本(Japan)" where 日本(Japan) is the answer we hope to get.
This means that the model get the answer correctly if the similarity of "vector(希腊) - vector(雅典) + vector(东京)" and vector(日本) is the largest among all words.

（we use the Chinese word analogy dataset introduced by (Chen et al., 2015), the dataset is divided into adjectives, nouns and verbs.
which consists of 1,124 tuples of words and each tuple contains
4 words, coming from three different categories:“Capital” (677 tuples), “State” (175 tuples), and“Family” (272 tuples). Our training corpus covers
all the testing words.）出自jwe

It contains 3 analogy types: (1) capitals of countries (687 groups); (2) states/provinces of cities (175 groups); and (3) family words (240 groups). The learning corpus covers
more than 97% of all the testing words

The dataset contains 3 groups of
analogy problems: capitals of countries, (China)
states/provinces of cities, and family relations.
////A word analogy compares the relationship between two pairs of words



4.3Case Study
In addition to the above quantitative analysis, we also conducted a qualitative analysis of the impact of implied meanings.





5 The Impacts of Parameter Settings

The parameter settings can affect the performance of word embeddings.
We take a task to investigate the impact of corpus size for word embeddings.
In the analysis of corpus size, we set the same hyperparameters as before.
We select the Wordsim-353 as the evaluation standard of word similarity.
The entire corpus previously mentioned are divided into 1/5,2/5,3/5,4/5 and 5/5 respectively as our new corpus for the task.
As shown in Fig. 4,



6 Conclusion and Future Work
In this paper,we explored a new direction of using the latent meanings of Chinese internal components instead of themselves to enhance the Chinese word embeddings.
We propose three models named MCWE-Avg,MCWE-SA and MCWE-HA which make full use of subword information.
The attention model is used to dynamically adjust the weights of latent meanings for the main-components of Chinese characters in MCWE-SA and MCWE-HA.


//we introduce latent meanings of Chinese character's main-compoent into word embedding methods to enhance Chinese word embeddings.




Acknowledgments
The authors are grateful to the reviewers for constructive feedback.
We would like to thank the anonymous reviewers for their insightful comments and suggestions.


References
















