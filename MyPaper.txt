Abstract  
We propose MCWE,a novel method for learning Chinese word embeddings.
Most Chinese word embeddings takes a word as the basic unit, or directly use the internal structure of words.
However,these models still ignore the rich implicit information inside Chinese.
In this paper,we explore to employ the latent meanings of the main components of the Chinese characters to train and enhance the word embeddings.
Based on this purpose,we propose a main-component enhanced word embedding model(MCWE),
which incorporate the latent meanings of the main-components during the training process.Experiments on word similarity, syntactic analogy and text classification are conducted to validate the feasibility of our model.
Evaluation on both word similarity and word analogy tasks demonstrates the superior performance of our model.

1.example:明智
  Word embedding, which is also termed distributed word representation, has been an important foundation in the field of Natural Language Processing (NLP).
It encodes the semantic meanings of a word into a real-valued low-dimensional vector,which perform better in many tasks such as text classification, machine translation ..... (and so on) over traditional one-hot representations.
Among many word vector models,Continuous Bag-of-Word (CBOW), Skip-gram (Mikolov et al.,2013a), Global Vectors (GloVe) (Penningtonet al., 2014) is popular because of their simplicity and efficiency.
(分布式假说)
However, these models only focus on word level information, and do not pay attention to the fine-grained morphological information inside the words,such as componets of Chinese character or English
morphemes.
  Different from English,Chinese characters are glyphs whose components may depict objects or represent abstract notions.Usually a character consists of more commonly two or more components which have meanings close to the character, using a variety of different principles[wiki written chinese]
Chinese words contain Chinese characters and subcharacter compoents,including rich semantic information.
However, the subcharacter compoents of a character contain a lot of unwanted noise information, so we integrate the word components into main-component.
Chinese characters contain elements and radicals，Just as Chinese contains Chinese characters.
The components of characters can be roughly divided into two types: main-component and radical.The main-component which consists of several components indicates the baisc meaning of a character while the radical indicates some attribute meanings of a character.
For example,"智 (intelligence)” is reduced into the components “矢(arrow)”, “口(mouth)” and “日(sun)”,and 矢(arrow) and 口(mouth) and are the components and 日(sun) is the radical of charactor 智.All these components may not be relevant to the semantics of the character.However, the main-component "知(wisdom)" consisting of several small components is closely related to the meaning of the charactor. Based on this, we have proposed .....
*******************************
Monday
In this work, we present a model to jointly learn the embeddings of Chinese words, characters,and subcharacter components.
(huang er al. Learning Chinese Word Representations From Glyphs Of Characters)proposed the GWE model to enhance the representation of words with character glyphs.
(Yu et al.Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components) used Chinese characters and subcharacter components to improve Chinese word embeddings 
and proposed the JWE model to jointly learn Chinese word and character embeddings.
(Cao et al. cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information)proposed a cw2vec model, which exploits stroke-level information to improve the learning of Chinese word embeddings.

2.My model(map表需要精确的标注隐含词义,对模型影响很大)
In this section, we introduce our Main-Component Word Embedding models(MCWEs),named Main-Component Word Embedding-Average(MCWE-Avg),MCWE-Soft Attention(MCWE-SA),MCWE-Hard Attention(MCWE-HA),
which is based on CBOW model(Mikolov et al., 2013a).
It should be noted that our models directly uses the internal meanings of the Chinese characters,rather than directly using the characters or componets of the word themselves.
MCWE-Avg assumes that all the latent meanings of the Chinese character's main-component to have the same weight.
However, most of the main-components are also a frequently-used Chinese word which may contain some ambiguous information.
To address this concern,we proposed the MCWE-SA which which based on the soft attention scheme.
The MCWE-SA assumes that the latent meanings of main-components have their own weights,and assign higher weights to the meanings closely related to the target character,
so that they have greater impact on the word embeddings.
What's more,MCWE-HA which based on the hard attention scheme only focuses on the latent meaning of the main-component with the highest similarity to the target word.
In what follows, we will introduce each of our MCWEs in detail.
At the end of this section, we are going to introduce the update rules of the models.

MCWE-Avg:

We denote the Chinese character set as C and the Chinese word vocabulary as W. Each character ci-C is represented by vector ci, 
and each word wi-W is represented by vector wi.

We denote D as the training corpus, W =(w1;w2;.....;wN) as the vocabulary of words,C = (c1; c2;.... ; cM) as the vocabulary of characters. 

The item for wi in the word map is wi -- Mi. Mi is a set of latent meanings of wi's main-components, 
(Particularly,we treat the radical as a main-component and add the latent meaning of the radical to the main-component latent meaning.)
Formally, a context word wj is represented as
公式见本子


MCWE-SA：

Through observation, we found that most Chinese words have more than one latent meaning with their main-component,
but some implicit meanings have low correlation with target words(the corresponding word).For example,........见图


*******************************
3.Experiments
In this section,we test the performance of our model in generating high-quality word embeddings on word similarity evaluation and word analogy tasks.

3.1 Experimental Settings

Traing Corpus : 
We utilize a medium-sized Chinese corpus to train all word embedding models.We adopt the Chinese Wikipedia Dump as our training corpus whose size is about 7GB.
We use a script named WikiExtractor to convert data from XML into text format.What's more,We use THULAC 3 (Sun et al.,2016b) for Chinese word segmentation and normalize all characters as simplified Chinese.
In pre-processing,we filter all digits,punctuation masks and non Chinese characters.
To get better quality of the word embeddings, we removed the stop words (that are common_ which from *********) in the corpus.
Finally, we obtained a training corpus of approximately 2g in size, containing 2123123 words, and 213 unique words.

Word Map:
The Modern Chinese Word List is divided into two parts: 2500 common characters and 1000 secondary common characters,and the coverage of those words in most corpora reached 99.48% which means mastering the common and secondary words has reached the basic requirements for using Chinese.
Hence,we use crawler script to obtain the main-components and radicals information of those Chinese characters which is a total of 3500 from HTTPCN. We obtained 3433 main-components and 57 radicals.
//we obtained 20,879 characters, 13,253 components and 218 radicals,of which 7,744 characters have more than one components, and 214 characters are equal to their radicals.
To create the word map,we need to obtain the explanatory meaning of the main-components of each characters.
Although the main-components are part of the characters, they are themselves characters.
Therefore, we have crawled the Chinese interpretation of 3433 main-component characters from HTTPCN and obtained the core latent meanings using manual annotation.
Similarly, we obtained the core latent meaning of 57 radicals from XXX.
Although this process costs manpower and time, it can be done once and for all for each languag because it has the same knowledge base.
When we choose a Chinese charactor,its main-component and radical will be selected and further replaced by their latent meanings.

(为了更快的训练速度 我们的模型采用了CBOW作为基础)

Parameter Settings:
(-cbow 1 -size 200 -window 5 -negative 10 -hs 0 -sample 1e-4 -threads 25 -binary 0 -iter 15 -alpha 0.025)
For the sake of fairness, we used the same parameter settings for all models.
In order to speed up the training process, we have adopted negative sampling techniques for both Cbow and skip-gram and our models.

We set the word vector dimension as 200, the window size as 5, the training iteration as 100, the initial learning rate as 0.025, and the subsampling parameter to be 1e-4. Words with
frequency less than 5 were ignored during training. We used 10-word negative sampling for optimization.

4.Evaluation Benchmarks
4.4.1 Word Similarity

This experiment is used to evaluate the semantic relevance of generated word embeddings in word pairs.

For Chinese word similarity, we employ two differnet manually-annotated datasets,wordsim-240 and wordsim-296 provided by (Chen et al., 2015).
These datasets are composed of word pairs and are manually labeled with the similarity scores for each word pair.
We utilize the cosine similarity to measure the similarity of each word pair, and the Spearman’s rank correlation coefficient (p) is employed to evaluate our calculation results and human scores.
More details of these datasets are shown in Table 1.


4.4.2 Syntactic Analogy

This task is to measure the quality of word embeddings by examining the semantic inference ability between word embeddings.
This
task examines the ability to deduce the semantic relations
between different words with the learned word embeddings.

A word analogy compares the relationship between two pairs of words









